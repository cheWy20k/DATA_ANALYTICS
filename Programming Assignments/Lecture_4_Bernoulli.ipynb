{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Programming Assignment\n",
    "#### Enrico Absin\n",
    "3. From the given code, modify the EM algorithm to become a Stochastic EM Algorithm.\n",
    "4. Use the data from the paper: https://www.sciencedirect.com/science/article/abs/pii/S0031320322001753\n",
    "5. Perform categorical clustering using the Bernoulli Mixture Model with Stochastic EM Algorithm.\n",
    "6. Compare its performance with K-Modes Algorithm using Folkes-Mallows Index, Adjusted Rand Index, and Normalized Mutual Information Score.\n",
    "7. Compare and contrast the performances, and explain what is happening (i.e. why is FMI always higher than ARI and NMI? Why is ARI and NMI low compared to FMI? etc.)\n",
    "8. Write the report in Latex, push to your github with the codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kmodes.kmodes import KModes\n",
    "from sklearn.metrics import fowlkes_mallows_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.metrics import fowlkes_mallows_score\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "class StochasticBernoulliMixture:\n",
    "    \n",
    "    def __init__(self, n_components, max_iter, batch_size=100, tol=1e-3):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.tol = tol\n",
    "    \n",
    "    def fit(self, x):\n",
    "        self.x = x\n",
    "        self.init_params()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.old_logL = self.get_log_likelihood(log_bernoullis)\n",
    "        for step in range(self.max_iter):\n",
    "            if step > 0:\n",
    "                self.old_logL = self.logL\n",
    "            \n",
    "            for batch in self.iterate_batches():\n",
    "                self.gamma = self.get_responsibilities(self.get_log_bernoullis(batch))\n",
    "                self.remember_params()\n",
    "                self.get_Neff()\n",
    "                self.get_mu(batch)\n",
    "                self.get_pi()\n",
    "                \n",
    "            log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "            self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "            if np.isnan(self.logL):\n",
    "                self.reset_params()\n",
    "                print(self.logL)\n",
    "                break\n",
    "\n",
    "    def iterate_batches(self):\n",
    "        n_samples = len(self.x)\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        for start_idx in range(0, n_samples, self.batch_size):\n",
    "            end_idx = min(start_idx + self.batch_size, n_samples)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            yield self.x.iloc[batch_indices]\n",
    "\n",
    "    def reset_params(self):\n",
    "        self.mu = self.old_mu.copy()\n",
    "        self.pi = self.old_pi.copy()\n",
    "        self.gamma = self.old_gamma.copy()\n",
    "        self.get_Neff()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "        \n",
    "    def remember_params(self):\n",
    "        self.old_mu = self.mu.copy()\n",
    "        self.old_pi = self.pi.copy()\n",
    "        self.old_gamma = self.gamma.copy()\n",
    "    \n",
    "    def init_params(self):\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.n_features = self.x.shape[1]\n",
    "        self.pi = 1/self.n_components * np.ones(self.n_components)\n",
    "        self.mu = np.random.RandomState(seed=0).uniform(low=0.25, high=0.75, size=(self.n_components, self.n_features))\n",
    "        self.normalize_mu()\n",
    "        self.old_mu = None\n",
    "        self.old_pi = None\n",
    "        self.old_gamma = None\n",
    "    \n",
    "    def normalize_mu(self):\n",
    "        sum_over_features = np.sum(self.mu, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            self.mu[k,:] /= sum_over_features[k]\n",
    "            \n",
    "    def get_responsibilities(self, log_bernoullis):\n",
    "        gamma = np.zeros(shape=(log_bernoullis.shape[0], self.n_components))\n",
    "        Z =  logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            gamma[:, k] = np.exp(np.log(self.pi[k]) + log_bernoullis[:,k] - Z)\n",
    "        return gamma\n",
    "        \n",
    "    def get_log_bernoullis(self, x):\n",
    "        log_bernoullis = self.get_save_single(x, self.mu)\n",
    "        log_bernoullis += self.get_save_single(1-x, 1-self.mu)\n",
    "        return log_bernoullis\n",
    "    \n",
    "    def get_save_single(self, x, mu):\n",
    "        epsilon = 1e-15\n",
    "        mu_place = np.clip(mu, epsilon, 1 - epsilon)\n",
    "        return np.tensordot(x, np.log(mu_place), (1,1))\n",
    "\n",
    "    def get_Neff(self):\n",
    "        self.Neff = np.sum(self.gamma, axis=0)\n",
    "    \n",
    "    def get_mu(self, batch):\n",
    "        self.mu = np.einsum('ik,id -> kd', self.gamma, batch) / self.Neff[:, None]\n",
    "        \n",
    "    def get_pi(self):\n",
    "        self.pi = self.Neff / self.n_samples\n",
    "    \n",
    "    def predict(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        gamma = self.get_responsibilities(log_bernoullis)\n",
    "        return np.argmax(gamma, axis=1)\n",
    "        \n",
    "    def get_sample_log_likelihood(self, log_bernoullis):\n",
    "        return logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "    \n",
    "    def get_log_likelihood(self, log_bernoullis):\n",
    "        return np.mean(self.get_sample_log_likelihood(log_bernoullis))\n",
    "        \n",
    "    def score(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_log_likelihood(log_bernoullis)\n",
    "    \n",
    "    def score_samples(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_sample_log_likelihood(log_bernoullis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 1, iteration: 1/100, moves: 18, cost: 149.0\n",
      "Run 1, iteration: 2/100, moves: 0, cost: 149.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 2, iteration: 1/100, moves: 24, cost: 157.0\n",
      "Run 2, iteration: 2/100, moves: 5, cost: 153.0\n",
      "Run 2, iteration: 3/100, moves: 3, cost: 150.0\n",
      "Run 2, iteration: 4/100, moves: 0, cost: 150.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 3, iteration: 1/100, moves: 25, cost: 155.0\n",
      "Run 3, iteration: 2/100, moves: 8, cost: 153.0\n",
      "Run 3, iteration: 3/100, moves: 4, cost: 152.0\n",
      "Run 3, iteration: 4/100, moves: 0, cost: 152.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 4, iteration: 1/100, moves: 16, cost: 139.0\n",
      "Run 4, iteration: 2/100, moves: 1, cost: 139.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 5, iteration: 1/100, moves: 7, cost: 168.0\n",
      "Run 5, iteration: 2/100, moves: 5, cost: 166.0\n",
      "Run 5, iteration: 3/100, moves: 0, cost: 166.0\n",
      "Best run was number 4\n"
     ]
    }
   ],
   "source": [
    "zoo_data = pd.read_csv(\"zoo.data\", header=None)\n",
    "\n",
    "X = zoo_data.iloc[:, 1:-1]\n",
    "y = zoo_data.iloc[:, -1]\n",
    "\n",
    "model_stochastic = StochasticBernoulliMixture(n_components=7, max_iter=100)\n",
    "model_stochastic.fit(X)\n",
    "\n",
    "model_kmodes = KModes(n_clusters=7, init='Huang', n_init=5, verbose=1)\n",
    "cluster_labels_kmodes = model_kmodes.fit_predict(X)\n",
    "\n",
    "fmi_stochastic = fowlkes_mallows_score(y, model_stochastic.predict(X))\n",
    "ari_stochastic = adjusted_rand_score(y, model_stochastic.predict(X))\n",
    "nmi_stochastic = normalized_mutual_info_score(y, model_stochastic.predict(X))\n",
    "\n",
    "fmi_kmodes = fowlkes_mallows_score(y, cluster_labels_kmodes)\n",
    "ari_kmodes = adjusted_rand_score(y, cluster_labels_kmodes)\n",
    "nmi_kmodes = normalized_mutual_info_score(y, cluster_labels_kmodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Bernoulli Mixture Model Performance:\n",
      "Folkes-Mallows Index (FMI): 0.48277252089435774\n",
      "Adjusted Rand Index (ARI): 0.0\n",
      "Normalized Mutual Information (NMI): 0.0\n",
      "\n",
      "KModes Algorithm Performance:\n",
      "Folkes-Mallows Index (FMI): 0.8766151924373514\n",
      "Adjusted Rand Index (ARI): 0.840144831114853\n",
      "Normalized Mutual Information (NMI): 0.8396495948660857\n"
     ]
    }
   ],
   "source": [
    "print(\"Stochastic Bernoulli Mixture Model Performance:\")\n",
    "print(\"Folkes-Mallows Index (FMI):\", fmi_stochastic)\n",
    "print(\"Adjusted Rand Index (ARI):\", ari_stochastic)\n",
    "print(\"Normalized Mutual Information (NMI):\", nmi_stochastic)\n",
    "\n",
    "print(\"\\nKModes Algorithm Performance:\")\n",
    "print(\"Folkes-Mallows Index (FMI):\", fmi_kmodes)\n",
    "print(\"Adjusted Rand Index (ARI):\", ari_kmodes)\n",
    "print(\"Normalized Mutual Information (NMI):\", nmi_kmodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Report\n",
    "#### Several conclusions may be drawn from a comparison of the KModes algorithm's and the Stochastic Bernoulli Mixture Model's performances on the Zoo dataset using the Folkes-Mallows Index (FMI), Adjusted Rand Index (ARI), and Normalized Mutual Information (NMI) metrics. In many situations, the pairwise similarity between clusters and actual classes—the emphasis of the FMI—is larger than the ARI and NMI. This is due to the fact that FMI can produce a better score even in cases when there are obvious pairwise similarities but overall grouping is not flawless. On the other hand, if the clustering structure is not in good alignment with the true classes overall, then ARI, which quantifies overall clustering similarity, may be lower than FMI.In addition, if there is a great deal of uncertainty in the clustering assignments or if the clustering and class assignments are not closely related, NMI—which calculates mutual information and normalization—may be lower than FMI.\n",
    "#### In particular, you can see which algorithm tends to have higher FMI values, whether ARI and NMI values are consistent between the two algorithms, and whether there are any situations where one algorithm performs better than the other based on these metrics when comparing the KModes algorithm and the Stochastic Bernoulli Mixture Model. These contrasts can shed light on the relative merits and shortcomings of each approach in terms of faithfully resolving the Zoo dataset's underlying clustering structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
